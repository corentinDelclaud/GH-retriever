Namespace(model_name='pt_llm', project='project_g_retriever', seed=1, dataset='expla_graphs', lr=1e-05, wd=0.05, patience=2, batch_size=8, grad_steps=2, num_epochs=10, warmup_epochs=1, eval_batch_size=16, llm_model_name='7b', llm_model_path='', llm_frozen='True', llm_num_virtual_tokens=10, output_dir='output', max_txt_len=512, max_new_tokens=32, max_memory=[40], gnn_model_name='gt2', gnn_num_layers=4, gnn_in_dim=1024, gnn_hidden_dim=1024, gnn_num_heads=4, gnn_dropout=0.0)
Loading LLAMA
Loading checkpoint shards: 100%|█████████████| 2/2 [00:01<00:00,  1.48it/s]
Freezing LLAMA!
Finish loading LLAMA!
trainable params: 40960 || all params: 6738456576 || trainable%: 0.0006078543289257816
 80%|███████████████████████████▏      | 1656/2070 [29:42<06:35,  1.05it/s]
Epoch: 0|10: Train Loss (Epoch Mean): 2.7594595197318257
Epoch: 0|10: Val Loss: 2.222287479468754
Saving checkpoint at epoch 0 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1_checkpoint_best.pth.
Epoch 0 Val Loss 2.222287479468754 Best Val Loss 2.222287479468754 Best Epoch 0
Epoch: 1|10: Train Loss (Epoch Mean): 1.4241653945710924
Epoch: 1|10: Val Loss: 0.8441291630268097
Saving checkpoint at epoch 1 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1_checkpoint_best.pth.
Epoch 1 Val Loss 0.8441291630268097 Best Val Loss 0.8441291630268097 Best Epoch 1
Epoch: 2|10: Train Loss (Epoch Mean): 0.5533534639699448
Epoch: 2|10: Val Loss: 0.4676422285182135
Saving checkpoint at epoch 2 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1_checkpoint_best.pth.
Epoch 2 Val Loss 0.4676422285182135 Best Val Loss 0.4676422285182135 Best Epoch 2
Epoch: 3|10: Train Loss (Epoch Mean): 0.3761467588120613
Epoch: 3|10: Val Loss: 0.4096267747027533
Saving checkpoint at epoch 3 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1_checkpoint_best.pth.
Epoch 3 Val Loss 0.4096267747027533 Best Val Loss 0.4096267747027533 Best Epoch 3
Epoch: 4|10: Train Loss (Epoch Mean): 0.345698825619071
Epoch: 4|10: Val Loss: 0.3984963717205184
Saving checkpoint at epoch 4 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1_checkpoint_best.pth.
Epoch 4 Val Loss 0.3984963717205184 Best Val Loss 0.3984963717205184 Best Epoch 4
Epoch: 5|10: Train Loss (Epoch Mean): 0.334511856669965
Epoch: 5|10: Val Loss: 0.388673717209271
Saving checkpoint at epoch 5 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1_checkpoint_best.pth.
Epoch 5 Val Loss 0.388673717209271 Best Val Loss 0.388673717209271 Best Epoch 5
Epoch: 6|10: Train Loss (Epoch Mean): 0.32605680204244053
Epoch: 6|10: Val Loss: 0.37730714678764343
Saving checkpoint at epoch 6 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1_checkpoint_best.pth.
Epoch 6 Val Loss 0.37730714678764343 Best Val Loss 0.37730714678764343 Best Epoch 6
Epoch: 7|10: Train Loss (Epoch Mean): 0.3180363351740123
Epoch: 7|10: Val Loss: 0.37584663948842456
Saving checkpoint at epoch 7 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1_checkpoint_best.pth.
Epoch 7 Val Loss 0.37584663948842456 Best Val Loss 0.37584663948842456 Best Epoch 7
Epoch: 8|10: Train Loss (Epoch Mean): 0.31309735537439154
Epoch: 8|10: Val Loss: 0.3679265084011214
Saving checkpoint at epoch 8 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1_checkpoint_best.pth.
Epoch 8 Val Loss 0.3679265084011214 Best Val Loss 0.3679265084011214 Best Epoch 8
Epoch: 9|10: Train Loss (Epoch Mean): 0.30741710737707534
Epoch: 9|10: Val Loss: 0.36106158771685193
Saving checkpoint at epoch 9 to output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1_checkpoint_best.pth.
Epoch 9 Val Loss 0.36106158771685193 Best Val Loss 0.36106158771685193 Best Epoch 9
  warnings.warn(
path: output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1.csv
Loading checkpoint from output/expla_graphs/model_name_pt_llm_llm_model_name_7b_llm_frozen_True_max_txt_len_512_max_new_tokens_32_gnn_model_name_gt2_patience_2_num_epochs_10_seed1_checkpoint_best.pth.
100%|██████████████████████████████████| 2070/2070 [38:35<00:00,  1.12s/it]
100%|██████████████████████████████████████| 35/35 [00:46<00:00,  1.33s/it]
Test Acc 0.555956678700361
/home/naeem/miniconda3/envs/g_retriever_HYBRID/lib/python3.9/site-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
